package prefetchers

import chisel3._
import chisel3.util._
import chisel3.experimental.{IO}
import freechips.rocketchip.config.{Config, Field, Parameters}
import freechips.rocketchip.rocket._
import freechips.rocketchip.rocket.constants.{MemoryOpConstants}
import freechips.rocketchip.tile.{BaseTile}
import freechips.rocketchip.subsystem.{CacheBlockBytes}
import freechips.rocketchip.diplomacy._

object HellaCachePrefetchWrapperFactory {
  def apply(hartIds: Seq[Int], prefetcher: CanInstantiatePrefetcher, base: BaseTile => Parameters => HellaCache) = (tile: BaseTile) => (p: Parameters) => {
    if (hartIds.contains(tile.staticIdForMetadataUseOnly)) {
      new HellaCachePrefetchWrapper(tile.staticIdForMetadataUseOnly, prefetcher, base(tile))(p)
    } else {
      base(tile)(p)
    }
  }
}

class HellaCachePrefetchWrapper(staticIdForMetadataUseOnly: Int, prefetcher: CanInstantiatePrefetcher, inner: Parameters => HellaCache)(implicit p: Parameters) extends HellaCache(staticIdForMetadataUseOnly)(p) {
  val cache = LazyModule(inner(p))
  override val node = cache.node
  override val hartIdSinkNodeOpt = cache.hartIdSinkNodeOpt
  override val mmioAddressPrefixSinkNodeOpt = cache.mmioAddressPrefixSinkNodeOpt
  override lazy val module = new HellaCachePrefetchWrapperModule(prefetcher, this) 
}

class HellaCachePrefetchWrapperModule(pP: CanInstantiatePrefetcher, outer: HellaCachePrefetchWrapper) extends HellaCacheModule(outer) with MemoryOpConstants{
  outer.cache.module.io <> io
  val cache = outer.cache.module

  val cycle_counter = RegInit(0.U(32.W))
  cycle_counter := cycle_counter + 1.U

  //Chicken bit
  val prefetcher_on = RegInit(false.B) 
  val matches_key = RegInit(false.B) 
  //If 32b'e11ae11a is written to the cache, stop prefetching
  matches_key := cache.io.cpu.resp.valid && isWrite(cache.io.cpu.resp.bits.cmd) && cache.io.cpu.resp.bits.store_data(31, 0) === "h_e11ae11a".asUInt()
  prefetcher_on := Mux(matches_key, !prefetcher_on, prefetcher_on)

  // Intercept and no-op prefetch requests generated by the core
  val core_prefetch = io.cpu.req.valid && isPrefetch(io.cpu.req.bits.cmd)
  when (io.cpu.req.valid && isPrefetch(io.cpu.req.bits.cmd)) {
    cache.io.cpu.req.valid := false.B
  }
  when (ShiftRegister(core_prefetch, 2)) {
    io.cpu.resp.valid := true.B
    io.cpu.s2_nack := false.B
    val req = ShiftRegister(io.cpu.req.bits, 2)
    val resp = io.cpu.resp.bits
    resp.addr := req.addr
    resp.tag := req.tag
    resp.cmd := req.cmd
    resp.size := req.size
    resp.signed := req.signed
    resp.dprv := req.dprv
    resp.data := req.data
    resp.mask := req.mask
    resp.replay := false.B
    resp.has_data := false.B
    resp.data_word_bypass := false.B
    resp.data_raw := false.B
    resp.store_data := false.B
  }
  when (cache.io.cpu.resp.valid && isPrefetch(cache.io.cpu.resp.bits.cmd)) {
    io.cpu.resp.valid := false.B
  }

  val prefetcher = pP.instantiate()

  prefetcher.io.snoop.valid := ShiftRegister(io.cpu.req.fire() && !core_prefetch, 2) && !io.cpu.s2_nack && !RegNext(io.cpu.s1_kill)
  prefetcher.io.snoop.bits.address := ShiftRegister(io.cpu.req.bits.addr, 2)
  prefetcher.io.snoop.bits.write := ShiftRegister(isWrite(io.cpu.req.bits.cmd), 2)

  //Connect up PC information to prefetcher
  prefetcher.io.pc.pc_1 := ShiftRegister(io.cpu.req.bits.data(23, 14), 2)
  prefetcher.io.pc.pc_0 := ShiftRegister(io.cpu.req.bits.data(13, 4), 2)
  prefetcher.io.pc.pc_valid_1 := ShiftRegister(io.cpu.req.bits.data(3), 2)
  prefetcher.io.pc.pc_valid_0 := ShiftRegister(io.cpu.req.bits.data(2), 2)
  prefetcher.io.pc.branch_taken_1 := ShiftRegister(io.cpu.req.bits.data(1), 2)
  prefetcher.io.pc.branch_taken_0 := ShiftRegister(io.cpu.req.bits.data(0), 2)

  //Flush until prefetcher is set on
  prefetcher.io.flush := !prefetcher_on

  val req = Queue(prefetcher.io.request, 1)
  val in_flight = RegInit(false.B)
  req.ready := false.B

  when (!io.cpu.req.valid) {
    // Only valid if in cacheable address space 
    cache.io.cpu.req.valid := req.valid && !in_flight && req.bits.address(31) && prefetcher_on
    cache.io.cpu.req.bits.addr := req.bits.block_address
    cache.io.cpu.req.bits.tag := 0.U
    cache.io.cpu.req.bits.cmd := Mux(req.bits.write, M_PFW, M_PFR)
    cache.io.cpu.req.bits.size := 0.U
    cache.io.cpu.req.bits.signed := false.B
    cache.io.cpu.req.bits.dprv := DontCare
    cache.io.cpu.req.bits.data := DontCare
    cache.io.cpu.req.bits.mask := DontCare
    cache.io.cpu.req.bits.phys := false.B
    cache.io.cpu.req.bits.no_alloc := false.B
    cache.io.cpu.req.bits.no_xcpt := false.B
    when (cache.io.cpu.req.fire()) { 
      in_flight := true.B 
    }
  }

  val prefetch_fire = cache.io.cpu.req.fire() && isPrefetch(cache.io.cpu.req.bits.cmd)
  when (ShiftRegister(prefetch_fire, 1)) {
    cache.io.cpu.s1_kill := false.B
  }
  when (ShiftRegister(prefetch_fire, 2)) {
    // HellaCache ignores DTLB prefetchable signal, so we recompute it here,
    // and kill the request in s2 if not prefetchable
    val paddr = cache.io.cpu.s2_paddr
    val legal = cache.edge.manager.findSafe(paddr).reduce(_||_)
    val prefetchable = cache.edge.manager.fastProperty(paddr, _.supportsAcquireT,
      (b: TransferSizes) => (!b.none).B)
    cache.io.cpu.s2_kill := !legal || !prefetchable
    req.ready := !cache.io.cpu.s2_nack
    in_flight := false.B
  }
  when (ShiftRegister(!io.cpu.req.valid, 2)) {
    io.cpu.s2_nack := false.B
  }
}

